# -*- coding: utf-8 -*-
"""Andri-FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eXGiM1F11CoyqCoFCu2HVdMNdDTn9Ha3

#Sentiment Analysis

##Scraping Data Twitter dengan keyword bemui

## Create data set
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

!pip install snscrape #untuk mengscrape data
import snscrape.modules.twitter as sntwitter #panggil snscrape modul twitter sebagai sntwitter
import itertools #generate scraping

import numpy as np
import csv

#scraping data with keyword bem ui
df = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(
   'bem ui').get_items(), 100))[['username', 'date', 'content']]
   #islice untuk memsiahkan kata, mencari data kata bem ui, dengan kolom username date dan conten sebanyak 100 baris

df.head()

df

"""##EDA"""

df.head(10)

df.tail(10)

df.info()

df.dtypes

df.describe()

df.isnull() #pengecekan null atau tidak, tidak null karna false

df.isna().sum() #handling missing value, untuk mengetahui nilai yg hilang,tidak ada data N/a

df.isnull().sum() #tidak ada data Null

df['content'] #gunakan untuk analisa sentimen

df = df.drop(columns=['username', 'date']) #menghapus kolom username dan date

df

df.content=df.content.astype(str) #analisa sentimen akan menghapus angka, maka harus dijadikan string dulu, maka jadilah dataset untuk analisis

df

df.to_csv('/content/drive/MyDrive/Colab Notebooks/dataset_bemui.csv', index=None) #save di drive dengan nama colab notebooks
#NLP untuk menganalisis text + = & natural
#masuk ke unsupervised learning

"""#Data cleaning & translating"""

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset_bemui.csv')
df

#  Cleaning text
import re #regex
import string #

def clean_text(tweet):
    tweet = tweet.lower() # create lowercase to all text
    tweet = re.sub('@[^\s]+', '', tweet) # remove usernames
    tweet = re.sub('\[.*?\]', '', tweet) # remove square brackets
    tweet = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', '', tweet) # remove URLs
    tweet = re.sub('[%s]' % re.escape(string.punctuation), '', tweet) # remove punctuation
    tweet = re.sub('\w*\d\w*', '', tweet) # remove character
    tweet = re.sub('[‘’“”…]', '', tweet) # remove character
    tweet = re.sub('\n', '', tweet) # remove character
    return tweet

    
tweet = lambda x: clean_text(x) #lambda untuk mengapply

df['clean1'] = pd.DataFrame(df.content.apply(tweet)) # nama kolom barunya clean1 untuk mengganti nama content lalu membuat dataframe dari df.content, lalu apply dari lambda tadi tweetnya
df

import nltk # Natural Language Toolkit
nltk.download('stopwords') # mendelete kata2 yang tidak penting, kata2 imbuhan
from nltk.corpus import stopwords # corpus adalah kamus, lalu panggil stopwords
additional  = ['rt','rts','retweet'] # kata yang ingin ditambahkan untuk dihapus
swords = set().union(stopwords.words('indonesian'), additional) # menghapus kalimat2 dari bahasa indonesia dan menambahkannya dengan additional

df['clean2'] = (df['clean1'].apply(lambda x: ' '.join([word for word in x.split() if word not in (swords)]))) #melakukan apply terhadap clean1, dengan membuat kolom baru,split untuk membagi atau memisahkan teks
df

text = df['clean2'] # menyimpan kolom data clean2 ke variabel text

df.dropna() #menghapus data kosong (N/a), output memnunjukkan tak ada data yg dihapus/ tak ada yg N/a

!pip install Sastrawi #sastrawi untuk stemming, untuk menghilangkan kata berimbuhan

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory #menggunakan stemming yg ada pada stemming

factory = StemmerFactory() #memanggil fungsi stemmerfactory
stemmer = factory.create_stemmer() #memanggil fungsi create stemmer pada stemmer factory

list_hasil = text

output = [(stemmer.stem(token)) for token in list_hasil] #untuk membuat tokenisasi, harus dibuat dalam bentuk pemisah kata

output

df['clean3'] = output #merubah var output jadi kolom clean3 dalam dataframe df

df

nltk.download('punkt') #nltk telah didownload karena True

df['tokens'] = pd.DataFrame(df['clean3'].apply(nltk.word_tokenize)) #gunakan fungsi dalam punkt untuk memisahkan kata dengan fungsi word_tokenoize
df #ngga kepake kolom tokens, cuma buat belajar doang shell ini

"""##Translate to english

"""

!pip install google_trans_new

from google_trans_new import google_translator  

translator = google_translator()

def translate_column(text, target_language): #fungsi untuk mentraslate kolom
    return translator.translate(text, lang_tgt=target_language) #translate ke dalam bahasa inggris

df['clean_english'] = df['clean3'].apply(lambda x: translate_column(x, 'en')) # mentranslate dengan target x,

df #kolom clean_english akan dilakukan analisis sentimen

df.to_csv('/content/drive/MyDrive/Colab Notebooks/dataset_bemui_clean_english3.csv', index=None)

"""#Modelling with unsupervised learning & visualization"""

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset_bemui_clean_english4.csv')
df.head()

df.isna().sum()

df['label'].value_counts() #menghitung jumlah +-/netral yg telah dibut secara manual

"""##Modelling with textblob"""

from textblob import TextBlob

df['clean_english'] = df['clean_english'].astype('str')
def get_polarity(text):
  return TextBlob(text).sentiment.polarity

df['polarity'] = df['clean_english'].apply(get_polarity)

df

df['sentimen_textblob']=''
df.loc[df.polarity>0,'sentimen_textblob']='positive'
df.loc[df.polarity==0,'sentimen_textblob']='neutral'
df.loc[df.polarity<0,'sentimen_textblob']='negative' #ejaan positif netral dan negatif harus sama dengan ejaan yang sebelumnya sudah dibuat pada kolom label

df

df['sentimen_textblob'].value_counts() #mengetahui jumlah negative positif dan neutral pada kolom sentiment_+textblob

df['label'].value_counts() #bandingkan dengan yang textblob

df.sentimen_textblob.value_counts().plot(kind='bar',title="Sentiment Analysis with Textblob") #visualisasi sentiment_textblob

df.sentimen_textblob.value_counts().plot(kind='pie',title="Sentiment Analysis with Textblob") #visualisasi sentimen_textblob

df.to_csv('/content/drive/MyDrive/Colab Notebooks/sentimen_textblobbemui.csv', index=None)

"""##Modelling with vader"""

from nltk.sentiment.vader import SentimentIntensityAnalyzer 
from nltk.sentiment.util import * #memanggil semua fungsi yang ada dalam sentiment util

import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer() #membuat alias untunk sentimenintesityanalyzer

df['scores'] = df['clean_english'].apply(lambda new_text: sid.polarity_scores(new_text)) #memanngil polarity, memanggilparameter new text
df

df['compound'] = df['scores'].apply(lambda score_dict: score_dict['compound']) #memanggil compound yang ada pada kolom scores
df['sentimen_vader']='' #membuat kolom baru
df.loc[df.compound>0,'sentimen_vader']='positive'
df.loc[df.compound==0,'sentimen_vader']='neutral'
df.loc[df.compound<0,'sentimen_vader']='negative'

df

df['sentimen_vader'].value_counts() #menghitung jumlah nilai sentimen vader

df['sentimen_textblob'].value_counts()

df['label'].value_counts()

df.sentimen_vader.value_counts().plot(kind='bar',title="Sentiment Analysis with Vader")

df.sentimen_vader.value_counts().plot(kind='pie',title="Sentiment Analysis with Vader")

df.to_csv('/content/drive/MyDrive/Colab Notebooks/sentimen_vaderbemui.csv', index=None)

"""# Menghitung Accuracy dengan sckit library learn / sk learn

##Acuraccy textblob
"""

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix #bentuk akurasi akan berupa matriks, dengan mengimport

accuracy_score(df['label'],df['sentimen_textblob']) #43% #menampilkan fungsi accuracy_score yang sudah diimport sebelumnya

print(classification_report(df['label'],df['sentimen_textblob'])) #menampilkan fungsiclassifikasi reporte yang sudah diimport sebelumnya
#presisi adalah jumlah yang diakulasi sebagai data yg benar, recall dari data yg dilabelin berapa yg negative benar
#f1-score rata2, support

#Confusion Matrix Testing, #menampilkan fungsi confusion_matrix yang sudah diimport sebelumnya, dengan var cm_pred cunclusion matrix predction
cm_pred = confusion_matrix(df['label'],df['sentimen_textblob'])
print(cm_pred)
#True + dan True -, matriks ukuran nya 3x3

df['sentimen_textblob'].value_counts()

import seaborn as sn
import matplotlib.pyplot as plt

df_cm_pred = pd.DataFrame(cm_pred, index = [i for i in ["Negative", "Neutral", "Positive"]],
                          columns = [i for i in ["Negative", "Neutral", "Positive"]])

plt.figure(figsize = (10,7))
sn.heatmap(df_cm_pred, annot=True, cbar=False, fmt="d")
plt.savefig('/content/drive/MyDrive/Colab Notebooks/confusion_matrix_textblob_bemui.jpg') #hasil textblob
#masih error

"""##Accuracy Vader"""

accuracy_score(df['label'],df['sentimen_vader']) #41%, vader lebih bagus daripada textblob dalam bentuk ketepatan

print(classification_report(df['label'],df['sentimen_vader']))

#Confusion Matrix Testing
cm_pred = confusion_matrix(df['label'],df['sentimen_vader'])
print(cm_pred)

import seaborn as sn
import matplotlib.pyplot as plt

df_cm_pred = pd.DataFrame(cm_pred, index = [i for i in ["Negative", "Neutral", "Positive"]],
                          columns = [i for i in ["Negative", "Neutral", "Positive"]])

plt.figure(figsize = (10,7))
sn.heatmap(df_cm_pred, annot=True, cbar=False, fmt="d")
plt.savefig('/content/drive/MyDrive/Colab Notebooks/confusion_matrix_vader.jpg')
# error

df.to_csv('/content/drive/MyDrive/Colab Notebooks/hasilcoba.csv', index=None) #hasil sentimen vader, untuk menghilangkan unammed kolom yg otomatis terbuat